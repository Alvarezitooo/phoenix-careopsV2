#!/usr/bin/env python3
"""
ğŸš€ SERVEUR RAG SIMPLE POUR PHOENIXCARE
Serveur Flask basique pour tester l'intÃ©gration frontend
"""

import os
import sys
import json
import hashlib
import time
import base64
from pathlib import Path
from datetime import datetime, timedelta
from difflib import SequenceMatcher
from collections import OrderedDict
from flask import Flask, request, jsonify
from flask_cors import CORS
from functools import wraps

# Ajout du chemin pour importer le RAG
sys.path.append(str(Path(__file__).parent / 'src'))

# Import du systÃ¨me RAG simple
import google.generativeai as genai
from dotenv import load_dotenv
import requests

# Chargement variables d'environnement
load_dotenv()

# Variables Supabase
SUPABASE_URL = os.getenv('NEXT_PUBLIC_SUPABASE_URL')
SUPABASE_ANON_KEY = os.getenv('NEXT_PUBLIC_SUPABASE_ANON_KEY')

app = Flask(__name__)

# ğŸ”’ CORS sÃ©curisÃ© - Uniquement origines autorisÃ©es
ALLOWED_ORIGINS = os.getenv('ALLOWED_ORIGINS', 'http://localhost:3000,http://localhost:3001').split(',')
CORS(app, origins=ALLOWED_ORIGINS, supports_credentials=True)

# ===== ğŸš€ SYSTÃˆME DE CACHE IN-MEMORY =====
class SmartCache:
    """Cache in-memory avec TTL et limite de taille"""
    def __init__(self, ttl_hours=24, max_size=1000):
        self.cache = OrderedDict()
        self.ttl = timedelta(hours=ttl_hours)
        self.max_size = max_size
        self.hits = 0
        self.misses = 0

    def _normalize_query(self, query: str) -> str:
        """Normalise la requÃªte pour amÃ©liorer le cache hit"""
        return query.lower().strip()

    def _get_hash(self, query: str) -> str:
        """Hash de la requÃªte normalisÃ©e (SHA256 pour sÃ©curitÃ©)"""
        normalized = self._normalize_query(query)
        return hashlib.sha256(normalized.encode()).hexdigest()

    def get(self, query: str):
        """RÃ©cupÃ¨re depuis le cache si valide"""
        cache_key = self._get_hash(query)

        if cache_key in self.cache:
            entry = self.cache[cache_key]
            if datetime.now() < entry['expires_at']:
                self.hits += 1
                # Move to end (LRU)
                self.cache.move_to_end(cache_key)
                print(f"âœ… Cache HIT ({self.hits} hits, {self.misses} misses)")
                return entry['data']
            else:
                del self.cache[cache_key]

        self.misses += 1
        print(f"âŒ Cache MISS ({self.hits} hits, {self.misses} misses)")
        return None

    def set(self, query: str, data: dict):
        """Stocke dans le cache avec TTL"""
        cache_key = self._get_hash(query)

        # Cleanup si trop grand
        if len(self.cache) >= self.max_size:
            # Supprimer le plus ancien (FIFO)
            self.cache.popitem(last=False)
            print(f"ğŸ§¹ Cache cleanup: {len(self.cache)}/{self.max_size}")

        self.cache[cache_key] = {
            'data': data,
            'expires_at': datetime.now() + self.ttl,
            'created_at': datetime.now()
        }

    def get_stats(self):
        """Stats du cache"""
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0
        return {
            'size': len(self.cache),
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': f"{hit_rate:.1f}%"
        }

# Instance globale du cache
cache = SmartCache(ttl_hours=24, max_size=1000)

# ===== ğŸ”’ RATE LIMITING SIMPLE =====
rate_limit_store = {}

def rate_limit(max_requests=10, window_minutes=1):
    """Rate limiting simple: max_requests par window_minutes"""
    def decorator(f):
        @wraps(f)
        def wrapped(*args, **kwargs):
            # RÃ©cupÃ©rer l'IP ou user_id
            ip = request.remote_addr
            user_id = request.get_json().get('user_id', ip) if request.get_json() else ip
            key = f"ratelimit:{user_id}"

            now = time.time()
            window_start = now - (window_minutes * 60)

            # Cleanup des anciennes entrÃ©es
            if key in rate_limit_store:
                rate_limit_store[key] = [t for t in rate_limit_store[key] if t > window_start]
            else:
                rate_limit_store[key] = []

            # VÃ©rifier la limite
            if len(rate_limit_store[key]) >= max_requests:
                return jsonify({
                    "error": "Trop de requÃªtes, rÃ©essayez dans quelques instants",
                    "retry_after": 60
                }), 429

            # Ajouter la requÃªte actuelle
            rate_limit_store[key].append(now)

            return f(*args, **kwargs)
        return wrapped
    return decorator

# ===== ğŸ›¡ï¸ SÃ‰CURITÃ‰ =====
def sanitize_input(text: str, max_length: int = 2000) -> str:
    """Nettoie et valide l'input utilisateur"""
    if not text:
        return ""

    # Limiter longueur
    text = str(text)[:max_length]

    # Enlever caractÃ¨res de contrÃ´le (garder \n \r)
    text = ''.join(char for char in text if char.isprintable() or char in '\n\r\t')

    # Ã‰chapper triples quotes qui cassent le prompt
    text = text.replace('"""', '').replace("'''", "")

    return text.strip()

def validate_context(context: dict) -> dict:
    """Valide et limite le contexte utilisateur"""
    if not isinstance(context, dict):
        return {}

    safe_context = {}

    # Limiter aides (max 10)
    if 'aides' in context and isinstance(context['aides'], list):
        safe_context['aides'] = context['aides'][:10]

    # Limiter deadlines (max 5)
    if 'deadlines' in context and isinstance(context['deadlines'], list):
        safe_context['deadlines'] = context['deadlines'][:5]

    # Limiter documents (max 5)
    if 'documents' in context and isinstance(context['documents'], list):
        safe_context['documents'] = context['documents'][:5]

    # Profil sÃ©curisÃ©
    if 'profile' in context and isinstance(context['profile'], dict):
        profile = context['profile']
        safe_context['profile'] = {
            'nb_enfants': int(profile.get('nb_enfants', 0)) if str(profile.get('nb_enfants', 0)).isdigit() else 0,
            'situation': sanitize_input(str(profile.get('situation', '')), 50)
        }

    return safe_context

# ===== ğŸ” AUTHENTIFICATION SUPABASE =====
def verify_supabase_token(token: str) -> dict:
    """VÃ©rifie le token Supabase et retourne les infos utilisateur"""
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None

    try:
        headers = {
            'apikey': SUPABASE_ANON_KEY,
            'Authorization': f'Bearer {token}'
        }
        response = requests.get(
            f"{SUPABASE_URL}/auth/v1/user",
            headers=headers,
            timeout=3
        )

        if response.status_code == 200:
            return response.json()
        return None
    except Exception as e:
        print(f"âš ï¸ Erreur vÃ©rification token: {e}")
        return None

def require_auth(f):
    """DÃ©corateur pour rendre l'auth obligatoire"""
    @wraps(f)
    def wrapped(*args, **kwargs):
        # RÃ©cupÃ©rer token
        auth_header = request.headers.get('Authorization', '')
        token = auth_header.replace('Bearer ', '').strip()

        if not token:
            return jsonify({
                "error": "Authentification requise",
                "code": "AUTH_REQUIRED"
            }), 401

        # VÃ©rifier token
        user_data = verify_supabase_token(token)
        if not user_data:
            return jsonify({
                "error": "Token invalide ou expirÃ©",
                "code": "INVALID_TOKEN"
            }), 401

        # Ajouter user_data au contexte de la requÃªte
        request.user_data = user_data
        return f(*args, **kwargs)

    return wrapped

# ===== ğŸ’­ MÃ‰MOIRE DE CONVERSATION SÃ‰CURISÃ‰E =====
conversation_memory = {}  # Format: {user_id: [messages]}
MAX_MEMORY_MESSAGES = 10
MAX_USERS_IN_MEMORY = 1000  # Limite globale

def get_conversation_history(user_id: str) -> list:
    """RÃ©cupÃ¨re l'historique de conversation (derniers 10 messages)"""
    return conversation_memory.get(user_id, [])[-MAX_MEMORY_MESSAGES:]

def add_to_conversation(user_id: str, message: str, response: str):
    """Ajoute un Ã©change Ã  l'historique avec limite globale"""
    # Cleanup si trop d'utilisateurs en mÃ©moire
    if len(conversation_memory) >= MAX_USERS_IN_MEMORY:
        # Supprimer l'utilisateur le plus ancien
        if conversation_memory:
            oldest_user = min(
                conversation_memory.items(),
                key=lambda x: x[1][-1].get('timestamp', '') if x[1] else ''
            )[0]
            del conversation_memory[oldest_user]
            print(f"ğŸ§¹ Cleanup mÃ©moire: suppression user {oldest_user}")

    if user_id not in conversation_memory:
        conversation_memory[user_id] = []

    conversation_memory[user_id].append({
        'user': sanitize_input(message, 500),  # Limiter taille en mÃ©moire
        'assistant': sanitize_input(response, 1000),
        'timestamp': datetime.now().isoformat()
    })

    # Cleanup si trop grand (garder dernier 10)
    if len(conversation_memory[user_id]) > MAX_MEMORY_MESSAGES:
        conversation_memory[user_id] = conversation_memory[user_id][-MAX_MEMORY_MESSAGES:]

def save_conversation_to_supabase(user_id: str, message: str, response: str, sources: list):
    """ğŸ’¾ Sauvegarde ou met Ã  jour la conversation dans Supabase"""
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return

    try:
        headers = {
            'apikey': SUPABASE_ANON_KEY,
            'Content-Type': 'application/json',
            'Prefer': 'return=representation'
        }

        # 1. Chercher si une conversation existe dÃ©jÃ  pour cet utilisateur
        search_response = requests.get(
            f"{SUPABASE_URL}/rest/v1/conversations?user_id=eq.{user_id}&select=id,messages",
            headers=headers,
            timeout=3
        )

        existing_conversations = search_response.json() if search_response.status_code == 200 else []

        # CrÃ©er le nouveau message
        new_message = {
            'role': 'user',
            'content': message,
            'timestamp': datetime.now().isoformat()
        }

        new_response_msg = {
            'role': 'assistant',
            'content': response,
            'sources': sources,
            'timestamp': datetime.now().isoformat()
        }

        if existing_conversations and len(existing_conversations) > 0:
            # 2. UPDATE : Ajouter les messages Ã  la conversation existante
            conversation = existing_conversations[0]
            conversation_id = conversation['id']
            existing_messages = conversation.get('messages', [])

            # Ajouter les nouveaux messages
            existing_messages.append(new_message)
            existing_messages.append(new_response_msg)

            # Limiter Ã  100 derniers messages pour Ã©viter trop de data
            if len(existing_messages) > 100:
                existing_messages = existing_messages[-100:]

            update_data = {
                'messages': existing_messages,
                'updated_at': datetime.now().isoformat()
            }

            update_response = requests.patch(
                f"{SUPABASE_URL}/rest/v1/conversations?id=eq.{conversation_id}",
                headers=headers,
                json=update_data,
                timeout=3
            )

            if update_response.status_code in [200, 204]:
                print(f"ğŸ’¾ Conversation mise Ã  jour ({len(existing_messages)} messages)")
            else:
                print(f"âš ï¸ Erreur update Supabase: {update_response.status_code}")

        else:
            # 3. INSERT : CrÃ©er une nouvelle conversation
            insert_data = {
                'user_id': user_id,
                'messages': [new_message, new_response_msg],
                'context': {},
                'created_at': datetime.now().isoformat()
            }

            insert_response = requests.post(
                f"{SUPABASE_URL}/rest/v1/conversations",
                headers=headers,
                json=insert_data,
                timeout=3
            )

            if insert_response.status_code in [200, 201]:
                print(f"ğŸ’¾ Nouvelle conversation crÃ©Ã©e dans Supabase")
            else:
                print(f"âš ï¸ Erreur insert Supabase: {insert_response.status_code}")

    except Exception as e:
        print(f"âš ï¸ Erreur sauvegarde Supabase: {e}")

# Configuration Gemini
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

generation_config = {
    "temperature": 0.7,  # AugmentÃ© pour des rÃ©ponses plus naturelles
    "top_p": 0.9,
    "top_k": 40,
    "max_output_tokens": 2000,
}

# ===== ğŸ”„ GÃ‰NÃ‰RATION GEMINI AVEC RETRY =====
def generate_with_gemini(prompt: str, max_retries: int = 3) -> str:
    """GÃ©nÃ¨re une rÃ©ponse avec Gemini avec retry automatique"""
    last_error = None

    for attempt in range(max_retries):
        try:
            # Timeout de 30 secondes
            response = model.generate_content(prompt, request_options={'timeout': 30})

            if response.text:
                return response.text
            else:
                raise ValueError("RÃ©ponse vide de Gemini")

        except Exception as e:
            last_error = e
            print(f"âš ï¸ Gemini attempt {attempt + 1}/{max_retries} failed: {e}")

            if attempt < max_retries - 1:
                # Attendre avant retry (backoff exponentiel)
                wait_time = 2 ** attempt
                print(f"â³ Retry dans {wait_time}s...")
                time.sleep(wait_time)
            else:
                print(f"âŒ Ã‰chec final aprÃ¨s {max_retries} tentatives")

    # Si tous les retries Ã©chouent
    error_msg = f"Erreur Gemini: {str(last_error)}"
    print(error_msg)
    return "Je rencontre des difficultÃ©s techniques pour rÃ©pondre. Veuillez rÃ©essayer dans quelques instants."

model = genai.GenerativeModel(
    model_name="models/gemini-2.5-flash",
    generation_config=generation_config,
    system_instruction="""Tu es PhoenixIA, conseiller social expert multi-domaines.

EXPERTISE COMPLÃˆTE:
- ğŸ›ï¸ MDPH: Toutes allocations handicap (AEEH, AAH, PCH), cartes, orientations
- ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ CAF: Allocations familiales, ARS, complÃ©ment familial, aides parentales
- ğŸ« SCOLARISATION: AESH, PPS, inclusion, transport scolaire adaptÃ©
- ğŸ‘” DROITS PARENTAUX: CongÃ©s aidant, AJPP, droits sociaux, recours
- ğŸš— MOBILITÃ‰: Cartes CMI, transports adaptÃ©s, aides techniques
- ğŸ“‹ PROCÃ‰DURES: Dossiers MDPH, CDAPH, recours, dÃ©lais

MISSION: Accompagner les familles comme un conseiller MDPH+CAF unifiÃ©.

STYLE: Empathique, prÃ©cis, concret. Toujours proposer des actions et Ã©tapes pratiques."""
)

# ===== ğŸ“š CHARGEMENT BASE DE CONNAISSANCES =====
def load_knowledge_base() -> dict:
    """
    ğŸ“š Charge la base de connaissances depuis config/knowledge_base.json

    Returns:
        dict: Base de connaissances chargÃ©e, ou {} si erreur
    """
    try:
        config_path = Path(__file__).parent / 'config' / 'knowledge_base.json'

        if not config_path.exists():
            print(f"âš ï¸ Fichier knowledge_base.json introuvable: {config_path}")
            print("ğŸ“ CrÃ©ez le fichier ou vÃ©rifiez le chemin")
            return {}

        with open(config_path, 'r', encoding='utf-8') as f:
            knowledge_base = json.load(f)

        print(f"âœ… Base de connaissances chargÃ©e: {len(knowledge_base)} documents")
        return knowledge_base

    except json.JSONDecodeError as e:
        print(f"âŒ Erreur JSON dans knowledge_base.json: {e}")
        return {}
    except Exception as e:
        print(f"âŒ Erreur chargement knowledge_base: {e}")
        return {}

# Chargement de la base de connaissances au dÃ©marrage
knowledge_base = load_knowledge_base()

def fuzzy_match(s1: str, s2: str) -> float:
    """Calcule similaritÃ© fuzzy entre deux strings (0-1)"""
    return SequenceMatcher(None, s1.lower(), s2.lower()).ratio()

def extract_suggestions(text: str) -> tuple[str, list]:
    """Extrait les suggestions de la rÃ©ponse IA"""
    if "SUGGESTIONS:" not in text:
        return text, []

    parts = text.split("SUGGESTIONS:")
    main_answer = parts[0].strip()

    suggestions = []
    if len(parts) > 1:
        suggestions_text = parts[1].strip()
        for line in suggestions_text.split('\n'):
            line = line.strip()
            if line.startswith('-'):
                suggestion = line[1:].strip()
                if suggestion:
                    suggestions.append(suggestion)

    return main_answer, suggestions[:3]  # Max 3 suggestions

def find_relevant_documents(query: str) -> list:
    """ğŸ” Recherche amÃ©liorÃ©e avec fuzzy matching dans la base de connaissances"""
    query_lower = query.lower()
    relevant_docs = []

    for doc_id, doc in knowledge_base.items():
        score = 0

        # 1. Score exact sur les mots-clÃ©s (prioritÃ© haute)
        for keyword in doc["keywords"]:
            if keyword in query_lower:
                score += 2.0

        # 2. Score fuzzy sur les mots-clÃ©s (tolÃ©rance fautes de frappe)
        for keyword in doc["keywords"]:
            query_words = query_lower.split()
            for word in query_words:
                if len(word) > 3:  # Ignore mots courts
                    similarity = fuzzy_match(word, keyword)
                    if similarity > 0.75:  # SimilaritÃ© > 75%
                        score += 1.5 * similarity

        # 3. Score basÃ© sur le contenu
        content_lower = doc["content"].lower()
        content_words = query_lower.split()
        for word in content_words:
            if len(word) > 3:
                # Exact match
                if word in content_lower:
                    score += 0.5
                # Fuzzy match dans le contenu
                else:
                    for content_word in content_lower.split():
                        if len(content_word) > 4:
                            similarity = fuzzy_match(word, content_word)
                            if similarity > 0.8:
                                score += 0.3 * similarity

        # 4. Bonus pour match dans le titre
        if any(word in doc["title"].lower() for word in query_lower.split() if len(word) > 3):
            score += 1.0

        if score > 0:
            relevant_docs.append({
                "id": doc_id,
                "title": doc["title"],
                "content": doc["content"],
                "score": round(score, 2)
            })

    # Tri par score dÃ©croissant
    relevant_docs.sort(key=lambda x: x["score"], reverse=True)
    return relevant_docs[:3]  # Top 3

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "service": "phoenix-rag-simple",
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/chat/send', methods=['POST'])
@rate_limit(max_requests=10, window_minutes=1)
@require_auth
def chat_send():
    """ğŸš€ Endpoint principal pour le chat RAG (avec cache + fuzzy search + contexte)"""
    start_time = time.time()

    try:
        data = request.get_json()

        # Validation des donnÃ©es
        if not data or 'message' not in data:
            return jsonify({"error": "Message requis"}), 400

        # ğŸ” SANITIZE INPUT
        message = sanitize_input(data['message'], max_length=2000)
        user_id = request.user_data.get('id', 'anonymous')  # Depuis le dÃ©corateur @require_auth

        # ğŸ” VALIDATE CONTEXT
        user_context = validate_context(data.get('context', {}))

        print(f"ğŸ“¨ RequÃªte reÃ§ue: {message[:50]}... (user: {user_id})")

        # ğŸš€ VÃ‰RIFIER LE CACHE D'ABORD
        cached_response = cache.get(message)
        if cached_response:
            print(f"âš¡ RÃ©ponse depuis le cache!")
            cached_response['from_cache'] = True
            return jsonify(cached_response)

        # ğŸ’­ RÃ©cupÃ©rer l'historique de conversation
        conversation_history = get_conversation_history(user_id)
        has_history = len(conversation_history) > 0

        # Recherche documents pertinents (avec fuzzy matching)
        relevant_docs = find_relevant_documents(message)

        # ğŸ“ PROMPT ENGINEERING AMÃ‰LIORÃ‰ (AVEC MÃ‰MOIRE + CONTEXTE)
        if relevant_docs:
            # Liste des sources avec scores
            sources_list = "\n".join([
                f"- {doc['title']} (pertinence: {doc['score']})"
                for doc in relevant_docs
            ])

            context = "\n\n---\n\n".join([
                f"ğŸ“„ SOURCE: {doc['title']}\n\n{doc['content']}"
                for doc in relevant_docs
            ])

            # Historique de conversation (si disponible)
            history_text = ""
            if has_history:
                history_text = "\nğŸ’­ HISTORIQUE RÃ‰CENT DE LA CONVERSATION:\n"
                for idx, exchange in enumerate(conversation_history[-3:], 1):  # Derniers 3 Ã©changes
                    history_text += f"{idx}. Utilisateur: {exchange['user']}\n"
                    history_text += f"   Toi: {exchange['assistant'][:100]}...\n\n"

            # Contexte utilisateur personnalisÃ©
            context_text = ""
            if user_context:
                context_text = "\nğŸ” CONTEXTE PERSONNEL DE L'UTILISATEUR:\n"
                if user_context.get('aides'):
                    context_text += "âœ… Aides actuelles:\n"
                    for aide in user_context['aides'][:5]:
                        context_text += f"  - {aide.get('nom', '')} ({aide.get('statut', '')})\n"
                if user_context.get('deadlines'):
                    context_text += "ğŸ“… Ã‰chÃ©ances importantes:\n"
                    for deadline in user_context['deadlines'][:3]:
                        context_text += f"  - {deadline.get('titre', '')} ({deadline.get('date', '')})\n"
                if user_context.get('profile'):
                    prof = user_context['profile']
                    if prof.get('nb_enfants'):
                        context_text += f"ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ Famille: {prof['nb_enfants']} enfant(s)\n"
                context_text += "\nâš ï¸ Utilise ce contexte pour personnaliser ta rÃ©ponse !\n"

            prompt = f"""Tu es PhoenixIA, conseiller social expert pour les familles d'enfants en situation de handicap.
{history_text}{context_text}
ğŸ“š SOURCES DISPONIBLES:
{sources_list}

ğŸ“„ DOCUMENTS DE RÃ‰FÃ‰RENCE:
{context}

â“ QUESTION ACTUELLE DE LA FAMILLE:
"{message}"

ğŸ“‹ INSTRUCTIONS DE RÃ‰PONSE:
1. RÃ©ponds de maniÃ¨re **empathique** et **rassurante**
2. Structure ta rÃ©ponse en **sections claires** (ex: Conditions, Montants, DÃ©marches)
3. **Cite systÃ©matiquement tes sources** entre parenthÃ¨ses
4. Utilise des **Ã©mojis** pour faciliter la lecture (âœ… âŒ ğŸ’° ğŸ“‹ âš ï¸)
5. Propose des **actions concrÃ¨tes** Ã  la fin
6. Si l'information n'est pas dans les documents, **dis-le clairement**
7. Si c'est une question de suivi, **fais rÃ©fÃ©rence Ã  l'historique**

âš ï¸ DISCLAIMER OBLIGATOIRE:
Termine TOUJOURS par: "â„¹ï¸ *Ces informations sont fournies Ã  titre indicatif. Pour une situation personnelle, contactez votre MDPH ou CAF.*"

ğŸ’¡ SUGGESTIONS DE QUESTIONS (IMPORTANT):
AprÃ¨s ta rÃ©ponse, ajoute EXACTEMENT 3 questions de suivi pertinentes que l'utilisateur pourrait vouloir poser.
Format : Sur une nouvelle ligne, Ã©cris "SUGGESTIONS:" suivi de 3 lignes commenÃ§ant par "- "

Exemple:
SUGGESTIONS:
- Comment faire ma demande MDPH en ligne ?
- Quels documents dois-je prÃ©parer ?
- Combien de temps prend le traitement du dossier ?

RÃ‰PONDS MAINTENANT:"""
        else:
            history_text = ""
            if has_history:
                history_text = "\nğŸ’­ HISTORIQUE RÃ‰CENT:\n"
                for idx, exchange in enumerate(conversation_history[-3:], 1):
                    history_text += f"{idx}. Utilisateur: {exchange['user']}\n"
                    history_text += f"   Toi: {exchange['assistant'][:100]}...\n\n"

            prompt = f"""Tu es PhoenixIA, conseiller social expert en droits du handicap en France.
{history_text}
â“ QUESTION: "{message}"

Tu n'as pas de documents spÃ©cifiques sur ce sujet, mais tu peux:
1. Donner des **informations gÃ©nÃ©rales** basÃ©es sur tes connaissances
2. Orienter vers les **bons organismes** (MDPH, CAF, associations)
3. Proposer de **reformuler** la question si nÃ©cessaire
4. Si c'est une question de suivi, **fais rÃ©fÃ©rence Ã  l'historique**

RÃ©ponds de maniÃ¨re **empathique** et **constructive** en franÃ§ais.

âš ï¸ Termine par: "â„¹ï¸ *Ces informations sont gÃ©nÃ©rales. Pour votre situation, contactez votre MDPH ou CAF.*"

ğŸ’¡ SUGGESTIONS:
AprÃ¨s ta rÃ©ponse, ajoute EXACTEMENT 3 questions de suivi sur une nouvelle ligne avec le format:
SUGGESTIONS:
- Question 1
- Question 2
- Question 3

RÃ‰PONDS:"""

        # ğŸ” GÃ©nÃ©ration avec Gemini (avec retry et timeout)
        full_response = generate_with_gemini(prompt, max_retries=3)

        # ğŸ’¡ Extraire suggestions de la rÃ©ponse
        answer, suggestions = extract_suggestions(full_response)

        # Sources utilisÃ©es
        sources = [doc['title'] for doc in relevant_docs] if relevant_docs else []

        processing_time = round(time.time() - start_time, 2)

        result = {
            "answer": answer,
            "response": answer,  # CompatibilitÃ©
            "conversation_id": f"conv_{user_id}_{int(datetime.now().timestamp())}",
            "sources": sources,
            "suggestions": suggestions,
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "from_cache": False,
            "search_scores": [doc['score'] for doc in relevant_docs]
        }

        # ğŸ’¾ STOCKER DANS LE CACHE
        cache.set(message, result)

        # ğŸ’­ AJOUTER Ã€ LA MÃ‰MOIRE DE CONVERSATION
        add_to_conversation(user_id, message, answer)

        # ğŸ’¾ SAUVEGARDER DANS SUPABASE (async, non-bloquant)
        try:
            save_conversation_to_supabase(user_id, message, answer, sources)
        except Exception as e:
            print(f"âš ï¸ Erreur sauvegarde Supabase (non-bloquant): {e}")

        print(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e: {len(answer)} chars, {len(sources)} sources, {processing_time}s")
        print(f"ğŸ“Š Cache stats: {cache.get_stats()}")
        print(f"ğŸ’­ MÃ©moire: {len(conversation_memory.get(user_id, []))} messages pour {user_id}")

        return jsonify(result)

    except Exception as e:
        print(f"âŒ Erreur RAG: {e}")
        return jsonify({
            "error": f"Erreur lors de la gÃ©nÃ©ration: {str(e)}",
            "answer": "Je rencontre une difficultÃ© technique. Pouvez-vous reformuler votre question ?",
            "sources": [],
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/api/cache/stats', methods=['GET'])
def cache_stats():
    """ğŸ“Š Statistiques du cache"""
    return jsonify(cache.get_stats())

@app.route('/api/memory/stats', methods=['GET'])
def memory_stats():
    """ğŸ’­ Statistiques de la mÃ©moire conversationnelle"""
    return jsonify({
        'users_count': len(conversation_memory),
        'total_messages': sum(len(msgs) for msgs in conversation_memory.values()),
        'max_per_user': MAX_MEMORY_MESSAGES
    })

@app.route('/api/memory/clear/<user_id>', methods=['DELETE'])
def clear_memory(user_id: str):
    """ğŸ—‘ï¸ Effacer la mÃ©moire d'un utilisateur"""
    if user_id in conversation_memory:
        del conversation_memory[user_id]
        return jsonify({'message': f'MÃ©moire effacÃ©e pour {user_id}'}), 200
    return jsonify({'message': 'Utilisateur non trouvÃ©'}), 404

@app.route('/api/summary/generate', methods=['POST'])
@require_auth
def generate_summary():
    """ğŸ“Š GÃ©nÃ©rer un rÃ©sumÃ© personnalisÃ© de la situation utilisateur"""
    try:
        data = request.get_json()

        if not data:
            return jsonify({"error": "DonnÃ©es requises"}), 400

        # ğŸ” VALIDATE CONTEXT
        user_context = validate_context(data.get('context', {}))
        user_id = request.user_data.get('id', 'anonymous')

        # Construction du prompt pour le rÃ©sumÃ©
        prompt = f"""Tu es PhoenixIA, conseiller social expert. GÃ©nÃ¨re un rÃ©sumÃ© CONCIS et ACTIONNABLE de la situation de cette famille.

ğŸ“Š DONNÃ‰ES DISPONIBLES:

âœ… AIDES ACTIVES:
"""

        if user_context.get('aides'):
            for aide in user_context['aides']:
                statut_emoji = 'âœ…' if aide.get('statut') == 'actif' else 'â³' if aide.get('statut') == 'en_attente' else 'âŒ'
                prompt += f"  {statut_emoji} {aide.get('nom', '')} - {aide.get('statut', '')} - {aide.get('montant', 'Montant non prÃ©cisÃ©')}\n"
        else:
            prompt += "  Aucune aide enregistrÃ©e\n"

        prompt += "\nğŸ“… Ã‰CHÃ‰ANCES IMPORTANTES:\n"
        if user_context.get('deadlines'):
            for deadline in user_context['deadlines']:
                prompt += f"  ğŸ“Œ {deadline.get('titre', '')} - {deadline.get('date', '')}\n"
        else:
            prompt += "  Aucune Ã©chÃ©ance enregistrÃ©e\n"

        prompt += "\nğŸ“„ DOCUMENTS:\n"
        if user_context.get('documents'):
            for doc in user_context['documents'][:5]:
                prompt += f"  ğŸ“„ {doc.get('nom', '')} - {doc.get('statut', '')}\n"
        else:
            prompt += "  Aucun document enregistrÃ©\n"

        prompt += "\nğŸ‘¨â€ğŸ‘©â€ğŸ‘§ PROFIL FAMILLE:\n"
        if user_context.get('profile'):
            prof = user_context['profile']
            prompt += f"  Enfants: {prof.get('nb_enfants', 'Non prÃ©cisÃ©')}\n"
            prompt += f"  Situation: {prof.get('situation', 'Non prÃ©cisÃ©e')}\n"
        else:
            prompt += "  Profil non rempli\n"

        prompt += """

ğŸ“‹ TON RÃ”LE:
GÃ©nÃ¨re un rÃ©sumÃ© en 4 sections COURTES (3-4 lignes max chacune):

1. **âœ… SITUATION ACTUELLE** - Ce qui est dÃ©jÃ  en place
2. **âš ï¸ POINTS D'ATTENTION** - Ce qui nÃ©cessite une action rapide (deadlines, documents manquants)
3. **ğŸ’¡ OPPORTUNITÃ‰S** - Aides potentielles auxquelles la famille pourrait avoir droit
4. **ğŸ¯ PROCHAINES Ã‰TAPES** - 3 actions concrÃ¨tes Ã  faire (numÃ©rotÃ©es)

Sois CONCIS, PRÃ‰CIS et ACTIONNABLE. Utilise des Ã©mojis et du markdown pour la lisibilitÃ©.
Termine par: "â„¹ï¸ *RÃ©sumÃ© gÃ©nÃ©rÃ© automatiquement. Pour plus de dÃ©tails, discutez avec Phoenix.*"
"""

        # ğŸ” GÃ©nÃ©ration avec Gemini (avec retry et timeout)
        summary = generate_with_gemini(prompt, max_retries=3)

        return jsonify({
            'summary': summary,
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        print(f"âŒ Erreur gÃ©nÃ©ration rÃ©sumÃ©: {e}")
        return jsonify({
            "error": f"Erreur lors de la gÃ©nÃ©ration: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/api/chat/analyze-document', methods=['POST'])
@require_auth
def analyze_document():
    """ğŸ“„ Analyser un document uploadÃ© par l'utilisateur avec Gemini Vision"""
    try:
        data = request.get_json()

        if not data:
            return jsonify({"error": "DonnÃ©es requises"}), 400

        # RÃ©cupÃ©ration et validation des donnÃ©es
        document_base64 = data.get('document', '')
        document_type = data.get('documentType', 'unknown')
        file_name = sanitize_input(data.get('fileName', 'document'), max_length=200)
        user_id = request.user_data.get('id', 'anonymous')

        if not document_base64:
            return jsonify({"error": "Document requis"}), 400

        print(f"ğŸ“„ Analyse document: {file_name} (type: {document_type}, user: {user_id})")

        # Extraction du contenu selon le type
        document_content = ""
        use_vision = False

        if document_type in ['scan', 'image']:
            # ğŸ–¼ï¸ IMAGES : Utiliser Gemini Vision
            print(f"ğŸ–¼ï¸ Analyse d'image avec Gemini Vision")
            use_vision = True

            # Retirer le prÃ©fixe data:image/...;base64,
            if 'base64,' in document_base64:
                document_base64 = document_base64.split('base64,')[1]

            # PrÃ©parer l'image pour Gemini Vision
            try:
                image_data = base64.b64decode(document_base64)
                # Gemini accepte directement les bytes d'image
                document_content = image_data
            except Exception as e:
                print(f"âš ï¸ Erreur dÃ©codage image: {e}")
                return jsonify({"error": "Format d'image invalide"}), 400

        elif document_type == 'pdf':
            # ğŸ“„ PDF : Extraire le texte avec PyPDF2
            print(f"ğŸ“„ Extraction PDF")
            try:
                # Retirer le prÃ©fixe data:application/pdf;base64,
                if 'base64,' in document_base64:
                    document_base64 = document_base64.split('base64,')[1]

                pdf_bytes = base64.b64decode(document_base64)

                # Sauvegarder temporairement le PDF
                import tempfile
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(pdf_bytes)
                    tmp_path = tmp_file.name

                # Extraire le texte avec PyPDF2
                try:
                    from PyPDF2 import PdfReader
                    reader = PdfReader(tmp_path)
                    text_parts = []
                    for page in reader.pages:
                        text_parts.append(page.extract_text())
                    document_content = "\n".join(text_parts)
                    document_content = sanitize_input(document_content, max_length=10000)
                finally:
                    # Nettoyer le fichier temporaire
                    import os as os_module
                    try:
                        os_module.unlink(tmp_path)
                    except:
                        pass

                if not document_content or len(document_content) < 50:
                    return jsonify({
                        "error": "PDF vide ou illisible",
                        "message": "Le PDF ne contient pas de texte extractible"
                    }), 400

            except ImportError:
                return jsonify({
                    "error": "PyPDF2 non installÃ©",
                    "message": "Installez PyPDF2 : pip install PyPDF2"
                }), 500
            except Exception as e:
                print(f"âš ï¸ Erreur extraction PDF: {e}")
                return jsonify({"error": f"Erreur extraction PDF: {str(e)}"}), 400

        else:
            # ğŸ“ TEXTE : DÃ©coder le base64
            try:
                # Retirer le prÃ©fixe data:text/plain;base64,
                if 'base64,' in document_base64:
                    document_base64 = document_base64.split('base64,')[1]

                decoded_bytes = base64.b64decode(document_base64)
                document_content = decoded_bytes.decode('utf-8', errors='ignore')
                document_content = sanitize_input(document_content, max_length=10000)
            except Exception as e:
                print(f"âš ï¸ Erreur dÃ©codage texte: {e}")
                return jsonify({"error": "Format de document invalide"}), 400

        # Validation finale pour texte/PDF
        if not use_vision and (not document_content or len(document_content) < 50):
            return jsonify({
                "error": "Document trop court ou vide",
                "message": "Veuillez uploader un document valide"
            }), 400

        # Construction du prompt d'analyse
        prompt = f"""Tu es PhoenixIA, expert en analyse de documents administratifs franÃ§ais liÃ©s au handicap et aux aides sociales.

ğŸ“„ DOCUMENT Ã€ ANALYSER:
Nom: {file_name}
Type: {document_type}

CONTENU:
{document_content[:5000]}

ğŸ“‹ TON RÃ”LE:
Analyse ce document en profondeur et fournis une rÃ©ponse structurÃ©e et ACTIONNABLE.

FORMAT DE RÃ‰PONSE (Markdown):

## ğŸ“‹ Type de document
[Identifie prÃ©cisÃ©ment: notification MDPH, certificat mÃ©dical, courrier CAF, dÃ©cision, attestation, etc.]

## ğŸ“… Informations clÃ©s extraites
- **Date du document**: [si prÃ©sente]
- **Organisme Ã©metteur**: [MDPH, CAF, CPAM, etc.]
- **Montants**: [si prÃ©sents]
- **Taux d'incapacitÃ©**: [si prÃ©sent]
- **Dates d'Ã©chÃ©ance/renouvellement**: [si prÃ©sentes]
- **DÃ©cision**: [AcceptÃ©e/RefusÃ©e/En attente]

## ğŸ“ RÃ©sumÃ© en langage simple
[Explique en 3-4 phrases ce que dit ce document, comme si tu parlais Ã  un parent qui ne connait pas le jargon administratif]

## âš ï¸ Points importants Ã  retenir
- [Point clÃ© 1]
- [Point clÃ© 2]
- [Point clÃ© 3]

## ğŸ¯ Actions recommandÃ©es
1. [Action concrÃ¨te 1 avec deadline si applicable]
2. [Action concrÃ¨te 2]
3. [Action concrÃ¨te 3]

## ğŸ’¡ Conseils Phoenix
[1-2 conseils personnalisÃ©s pour optimiser la situation ou Ã©viter les erreurs courantes]

IMPORTANT:
- Sois TRÃˆS PRÃ‰CIS sur les dates et montants
- Si des infos manquent, dis-le clairement
- Utilise un ton empathique et rassurant
- Mets en avant ce qui est URGENT

RÃ‰PONDS MAINTENANT:"""

        # GÃ©nÃ©ration avec Gemini (texte ou vision selon le type)
        if use_vision:
            # ğŸ–¼ï¸ Utiliser Gemini Vision pour analyser l'image
            print("ğŸ–¼ï¸ Analyse avec Gemini Vision...")
            try:
                import PIL.Image
                import io

                # Convertir bytes en PIL Image
                image = PIL.Image.open(io.BytesIO(document_content))

                # Utiliser gemini-2.0-flash-exp pour vision
                vision_model = genai.GenerativeModel('gemini-2.0-flash-exp')

                response = vision_model.generate_content(
                    [prompt, image],
                    request_options={'timeout': 30}
                )

                analysis = response.text if response.text else "Impossible d'analyser l'image."

            except ImportError:
                return jsonify({
                    "error": "PIL non installÃ©",
                    "message": "Installez Pillow : pip install Pillow"
                }), 500
            except Exception as e:
                print(f"âŒ Erreur Gemini Vision: {e}")
                return jsonify({
                    "error": f"Erreur analyse image: {str(e)}"
                }), 500
        else:
            # ğŸ“ Utiliser Gemini texte standard avec retry
            analysis = generate_with_gemini(prompt, max_retries=3)

        # Extraction de suggestions d'actions pour UI
        suggestions = []
        if "Actions recommandÃ©es" in analysis:
            # Extraire les numÃ©ros d'actions comme suggestions
            import re
            actions = re.findall(r'\d+\.\s*([^\n]+)', analysis)
            suggestions = actions[:3]  # Max 3 suggestions

        return jsonify({
            'analysis': analysis,
            'fullAnalysis': analysis,
            'suggestions': suggestions,
            'fileName': file_name,
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        print(f"âŒ Erreur analyse document: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "error": f"Erreur lors de l'analyse: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/api/user-memories', methods=['POST'])
@require_auth
def save_user_memory():
    """ğŸ§  Sauvegarder une mÃ©moire importante de l'utilisateur"""
    try:
        data = request.get_json()

        if not data or 'memory_content' not in data:
            return jsonify({"error": "Contenu de mÃ©moire requis"}), 400

        user_id = request.user_data.get('id')
        memory_content = sanitize_input(data['memory_content'], max_length=500)
        memory_type = data.get('memory_type', 'general')  # general, family, aide, deadline
        importance_score = min(max(int(data.get('importance_score', 5)), 1), 10)

        if not SUPABASE_URL or not SUPABASE_ANON_KEY:
            return jsonify({"error": "Supabase non configurÃ©"}), 500

        print(f"ğŸ§  Sauvegarde mÃ©moire pour {user_id}: {memory_content[:50]}...")

        # Sauvegarder dans Supabase
        headers = {
            'apikey': SUPABASE_ANON_KEY,
            'Content-Type': 'application/json',
            'Prefer': 'return=representation'
        }

        memory_data = {
            'user_id': user_id,
            'memory_content': memory_content,
            'memory_type': memory_type,
            'importance_score': importance_score,
            'created_at': datetime.now().isoformat()
        }

        response = requests.post(
            f"{SUPABASE_URL}/rest/v1/user_memories",
            headers=headers,
            json=memory_data,
            timeout=3
        )

        if response.status_code in [200, 201]:
            print(f"âœ… MÃ©moire sauvegardÃ©e")
            return jsonify({
                "success": True,
                "message": "MÃ©moire sauvegardÃ©e avec succÃ¨s"
            })
        else:
            print(f"âš ï¸ Erreur sauvegarde mÃ©moire: {response.status_code}")
            return jsonify({"error": "Erreur sauvegarde"}), 500

    except Exception as e:
        print(f"âŒ Erreur save_user_memory: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/user-memories/<user_id>', methods=['GET'])
@require_auth
def get_user_memories(user_id: str):
    """ğŸ§  RÃ©cupÃ©rer les mÃ©moires d'un utilisateur"""
    try:
        # VÃ©rifier que l'utilisateur demande ses propres mÃ©moires
        requesting_user_id = request.user_data.get('id')
        if requesting_user_id != user_id:
            return jsonify({"error": "AccÃ¨s non autorisÃ©"}), 403

        if not SUPABASE_URL or not SUPABASE_ANON_KEY:
            return jsonify({"memories": []}), 200

        headers = {
            'apikey': SUPABASE_ANON_KEY,
            'Content-Type': 'application/json'
        }

        # RÃ©cupÃ©rer les mÃ©moires triÃ©es par importance puis date
        response = requests.get(
            f"{SUPABASE_URL}/rest/v1/user_memories?user_id=eq.{user_id}&order=importance_score.desc,created_at.desc&limit=50",
            headers=headers,
            timeout=3
        )

        if response.status_code == 200:
            memories = response.json()
            return jsonify({"memories": memories})
        else:
            return jsonify({"memories": []}), 200

    except Exception as e:
        print(f"âŒ Erreur get_user_memories: {e}")
        return jsonify({"memories": []}), 200

@app.route('/api/user-memories/<memory_id>', methods=['DELETE'])
@require_auth
def delete_user_memory(memory_id: str):
    """ğŸ—‘ï¸ Supprimer une mÃ©moire"""
    try:
        user_id = request.user_data.get('id')

        if not SUPABASE_URL or not SUPABASE_ANON_KEY:
            return jsonify({"error": "Supabase non configurÃ©"}), 500

        headers = {
            'apikey': SUPABASE_ANON_KEY,
            'Content-Type': 'application/json'
        }

        # Supprimer seulement si c'est la mÃ©moire de l'utilisateur
        response = requests.delete(
            f"{SUPABASE_URL}/rest/v1/user_memories?id=eq.{memory_id}&user_id=eq.{user_id}",
            headers=headers,
            timeout=3
        )

        if response.status_code in [200, 204]:
            return jsonify({"success": True, "message": "MÃ©moire supprimÃ©e"})
        else:
            return jsonify({"error": "Erreur suppression"}), 500

    except Exception as e:
        print(f"âŒ Erreur delete_user_memory: {e}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸš€ SERVEUR RAG PHOENIX - VERSION OPTIMISÃ‰E V2")
    print("=" * 60)
    print(f"ğŸ“š Base de connaissances: {len(knowledge_base)} documents")
    print(f"ğŸ”‘ Gemini API: {'âœ…' if os.getenv('GEMINI_API_KEY') else 'âŒ'}")
    print(f"ğŸ” Supabase: {'âœ…' if SUPABASE_URL else 'âŒ (optionnel)'}")
    print(f"ğŸ’¾ Cache: TTL 24h, Max 1000 entrÃ©es")
    print(f"ğŸ” Recherche: Fuzzy matching activÃ© (difflib)")
    print(f"ğŸ”’ Rate limit: 10 req/min par utilisateur")
    print(f"ğŸ’­ MÃ©moire: {MAX_MEMORY_MESSAGES} derniers messages par user")
    print(f"ğŸ“ Prompt: Engineering avancÃ© avec contexte")
    print("=" * 60)
    print("ğŸ“ Endpoints disponibles:")
    print("  POST /api/chat/send - Chat principal")
    print("  GET  /api/cache/stats - Stats cache")
    print("  GET  /api/memory/stats - Stats mÃ©moire")
    print("  DELETE /api/memory/clear/<user_id> - Effacer mÃ©moire")
    print("=" * 60)

    # ğŸ” SECURITY: debug=True interdit en production (CWE-94)
    DEBUG_MODE = os.getenv('FLASK_DEBUG', 'False').lower() == 'true'
    NODE_ENV = os.getenv('NODE_ENV', 'development')

    if NODE_ENV == 'production' and DEBUG_MODE:
        raise RuntimeError("â›” SECURITÃ‰: FLASK_DEBUG=True est interdit en production! (risque code execution)")

    PORT = int(os.getenv('RAG_PORT', 8000))
    HOST = '0.0.0.0' if NODE_ENV == 'production' else '127.0.0.1'

    print(f"ğŸ” Debug mode: {'âœ… ActivÃ© (dev only)' if DEBUG_MODE else 'âŒ DÃ©sactivÃ© (production safe)'}")
    print(f"ğŸŒ Environment: {NODE_ENV}")
    print(f"ğŸ“ Listening on: {HOST}:{PORT}")

    app.run(host=HOST, port=PORT, debug=DEBUG_MODE)