"""
PhoenixRAG optimis√© pour Gemini 1.5 Flash
Syst√®me RAG simplifi√© et efficace pour PhoenixCare
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime
import json
import os
import google.generativeai as genai
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class RAGResponse:
    """R√©ponse du syst√®me RAG"""
    answer: str
    sources: List[Dict[str, Any]]
    confidence: float
    processing_time: float
    metadata: Dict[str, Any]

class PhoenixRAGSystem:
    """
    Syst√®me RAG PhoenixCare optimis√© pour Gemini 1.5 Flash
    """

    def __init__(self):
        self._init_gemini()
        self.conversation_history = {}

    def _init_gemini(self):
        """Initialise Gemini avec la cl√© API partag√©e"""
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY non trouv√©e dans les variables d'environnement")

        genai.configure(api_key=api_key)

        # Configuration optimis√©e pour PhoenixCare
        generation_config = {
            "temperature": 0.3,  # R√©ponses factuelles et coh√©rentes
            "top_p": 0.8,
            "top_k": 40,
            "max_output_tokens": 1000,  # R√©ponses d√©taill√©es mais concises
        }

        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        ]

        self.model = genai.GenerativeModel(
            model_name="gemini-1.5-flash",
            generation_config=generation_config,
            safety_settings=safety_settings,
            system_instruction=self._get_system_prompt()
        )

    def _get_system_prompt(self) -> str:
        """Prompt syst√®me sp√©cialis√© PhoenixCare"""
        return """Tu es PhoenixIA, assistant expert en droit du handicap fran√ßais.

Tu aides les parents d'enfants en situation de handicap avec des informations pr√©cises, bienveillantes et accessibles.

EXPERTISE:
- Code de l'Action Sociale et des Familles (CASF)
- AEEH (Allocation d'√âducation de l'Enfant Handicap√©)
- PCH (Prestation de Compensation du Handicap)
- Proc√©dures MDPH et CDAPH
- Scolarisation inclusive et AESH
- Droits et d√©marches administratives

STYLE DE R√âPONSE:
- Ton empathique et rassurant
- Langage accessible (√©vite le jargon)
- R√©ponses structur√©es et actionnables
- Citations des sources juridiques pr√©cises
- Propositions d'actions concr√®tes

LIMITATIONS:
- Tu ne remplaces pas une consultation juridique
- Tu ne peux pas pr√©dire les d√©cisions CDAPH
- Tu encourages toujours √† contacter les professionnels

STRUCTURE:
1. R√©ponse directe √† la question
2. Base juridique (articles, d√©crets)
3. Actions recommand√©es
4. Ressources compl√©mentaires"""

    async def generate_response(
        self,
        query: str,
        search_results: List[Dict[str, Any]] = None,
        intent_analysis: Dict[str, Any] = None,
        user_context: Dict[str, Any] = None
    ) -> RAGResponse:
        """
        G√©n√®re une r√©ponse compl√®te avec Gemini
        """
        start_time = datetime.utcnow()

        try:
            # Construction du prompt contextualis√©
            prompt = self._build_contextualized_prompt(
                query, search_results, intent_analysis, user_context
            )

            # G√©n√©ration avec Gemini
            response = await self._generate_with_gemini(prompt)

            # Extraction des m√©tadonn√©es de r√©ponse
            metadata = self._extract_response_metadata(response, intent_analysis)

            # Calcul du temps de traitement
            processing_time = (datetime.utcnow() - start_time).total_seconds()

            return RAGResponse(
                answer=response,
                sources=search_results or [],
                confidence=metadata.get('confidence', 0.8),
                processing_time=processing_time,
                metadata=metadata
            )

        except Exception as e:
            logger.error(f"Erreur g√©n√©ration r√©ponse: {e}")
            return RAGResponse(
                answer="Je rencontre une difficult√© technique. Veuillez reformuler votre question ou contacter le support.",
                sources=[],
                confidence=0.0,
                processing_time=(datetime.utcnow() - start_time).total_seconds(),
                metadata={"error": str(e)}
            )

    def _build_contextualized_prompt(
        self,
        query: str,
        search_results: List[Dict[str, Any]] = None,
        intent_analysis: Dict[str, Any] = None,
        user_context: Dict[str, Any] = None
    ) -> str:
        """
        Construit un prompt contextualis√© pour Gemini
        """
        prompt_parts = []

        # Contexte utilisateur si disponible
        if user_context:
            context_info = []
            if user_context.get('department'):
                context_info.append(f"D√©partement: {user_context['department']}")
            if user_context.get('child_age'):
                context_info.append(f"√Çge de l'enfant: {user_context['child_age']} ans")
            if user_context.get('disability_type'):
                context_info.append(f"Situation: {user_context['disability_type']}")

            if context_info:
                prompt_parts.append(f"CONTEXTE UTILISATEUR:\n{chr(10).join(context_info)}")

        # Analyse d'intention
        if intent_analysis:
            urgency = intent_analysis.get('urgency', 'normal')
            if urgency == 'high':
                prompt_parts.append("‚ö†Ô∏è DEMANDE URGENTE - Priorise les actions imm√©diates")

            if intent_analysis.get('requires_documents'):
                prompt_parts.append("üìã INCLURE: Documents et formulaires n√©cessaires")

        # Documents trouv√©s
        if search_results:
            sources_text = []
            for i, result in enumerate(search_results[:5], 1):
                metadata = result.get('metadata', {})
                content = result.get('content', result.get('excerpt', ''))

                source_info = f"SOURCE {i}:"
                if metadata.get('title'):
                    source_info += f"\nTitre: {metadata['title']}"
                if metadata.get('article'):
                    source_info += f"\nArticle: {metadata['article']}"
                if metadata.get('date'):
                    source_info += f"\nDate: {metadata['date']}"
                source_info += f"\nContenu: {content[:500]}..."

                sources_text.append(source_info)

            prompt_parts.append(f"DOCUMENTS JURIDIQUES PERTINENTS:\n{chr(10).join(sources_text)}")

        # Question utilisateur
        prompt_parts.append(f"QUESTION: {query}")

        # Instructions finales
        prompt_parts.append("""
INSTRUCTIONS:
1. R√©ponds en fran√ßais, de mani√®re empathique
2. Base ta r√©ponse sur les documents fournis
3. Cite tes sources (articles, dates)
4. Propose des actions concr√®tes
5. Indique si l'info est sp√©cifique √† une r√©gion
6. Structure ta r√©ponse clairement
7. Si incertain, dis-le explicitement

R√âPONSE:""")

        return "\n\n".join(prompt_parts)

    async def _generate_with_gemini(self, prompt: str) -> str:
        """
        G√©n√®re une r√©ponse avec Gemini de mani√®re asynchrone
        """
        try:
            # Gemini n'a pas d'API async native, on simule avec asyncio
            response = await asyncio.to_thread(
                self.model.generate_content, prompt
            )

            if response.candidates and response.candidates[0].content.parts:
                return response.candidates[0].content.parts[0].text
            else:
                return "Je ne peux pas g√©n√©rer une r√©ponse appropri√©e pour cette question."

        except Exception as e:
            logger.error(f"Erreur Gemini: {e}")
            raise

    def _extract_response_metadata(
        self,
        response: str,
        intent_analysis: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Extrait les m√©tadonn√©es de la r√©ponse g√©n√©r√©e
        """
        metadata = {
            'response_length': len(response),
            'model_used': 'gemini-1.5-flash',
            'timestamp': datetime.utcnow().isoformat()
        }

        # Score de confiance bas√© sur la longueur et structure
        confidence = 0.8  # Base

        if len(response) > 200:  # R√©ponse d√©taill√©e
            confidence += 0.1

        if any(word in response.lower() for word in ['article', 'code', 'd√©cret']):
            confidence += 0.1  # R√©f√©rences juridiques

        if any(word in response.lower() for word in ['mdph', 'aeeh', 'pch']):
            confidence += 0.05  # Termes sp√©cialis√©s

        metadata['confidence'] = min(1.0, confidence)

        # Analyse d'intention int√©gr√©e
        if intent_analysis:
            metadata['intent_analysis'] = intent_analysis

        return metadata

    async def stream_response(
        self,
        query: str,
        search_results: List[Dict[str, Any]] = None,
        intent_analysis: Dict[str, Any] = None
    ) -> AsyncGenerator[str, None]:
        """
        G√©n√®re une r√©ponse en streaming (simulation pour Gemini)
        """
        # Pour l'instant, Gemini n'a pas de streaming natif
        # On simule en d√©coupant la r√©ponse
        full_response = await self.generate_response(query, search_results, intent_analysis)

        # D√©coupage de la r√©ponse en chunks
        words = full_response.answer.split()
        chunk_size = 3  # 3 mots par chunk

        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            if i + chunk_size < len(words):
                chunk += " "
            yield chunk
            await asyncio.sleep(0.1)  # Simulation d√©lai streaming

    def add_conversation_context(self, conversation_id: str, message: str, role: str = "user"):
        """
        Ajoute un message au contexte conversationnel
        """
        if conversation_id not in self.conversation_history:
            self.conversation_history[conversation_id] = []

        self.conversation_history[conversation_id].append({
            'role': role,
            'content': message,
            'timestamp': datetime.utcnow().isoformat()
        })

        # Limite √† 10 derniers messages pour √©viter de surcharger le contexte
        if len(self.conversation_history[conversation_id]) > 10:
            self.conversation_history[conversation_id] = self.conversation_history[conversation_id][-10:]

    def get_conversation_context(self, conversation_id: str) -> str:
        """
        R√©cup√®re le contexte conversationnel format√©
        """
        if conversation_id not in self.conversation_history:
            return ""

        messages = self.conversation_history[conversation_id][-5:]  # 5 derniers messages
        context_parts = []

        for msg in messages:
            role = "Utilisateur" if msg['role'] == 'user' else "PhoenixIA"
            context_parts.append(f"{role}: {msg['content']}")

        return "\n".join(context_parts)

# Instance globale
phoenix_rag = PhoenixRAGSystem()