"""
Module de conformit√© l√©gale pour le scraping PhoenixCare
Respect des r√®gles juridiques fran√ßaises et europ√©ennes
"""

import asyncio
import aiohttp
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse
import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
import time

logger = logging.getLogger(__name__)

class LegalComplianceChecker:
    """
    V√©rificateur de conformit√© l√©gale pour le scraping
    """

    # Sites officiels fran√ßais autoris√©s explicitement
    AUTHORIZED_GOVERNMENT_SITES = {
        'legifrance.gouv.fr': {
            'type': 'official_legal',
            'open_data': True,
            'notes': 'Donn√©es juridiques publiques de la R√©publique fran√ßaise'
        },
        'cnsa.fr': {
            'type': 'official_health',
            'open_data': True,
            'notes': 'Caisse nationale de solidarit√© pour l\'autonomie'
        },
        'service-public.fr': {
            'type': 'official_service',
            'open_data': True,
            'notes': 'Site officiel de l\'administration fran√ßaise'
        },
        'handicap.gouv.fr': {
            'type': 'official_handicap',
            'open_data': True,
            'notes': 'Secr√©tariat d\'√âtat charg√© des Personnes handicap√©es'
        },
        'data.gouv.fr': {
            'type': 'open_data',
            'open_data': True,
            'notes': 'Plateforme ouverte des donn√©es publiques fran√ßaises'
        }
    }

    # Patterns d'URLs √† √©viter (donn√©es personnelles potentielles)
    FORBIDDEN_PATTERNS = [
        r'/user/',
        r'/profile/',
        r'/private/',
        r'/admin/',
        r'/compte/',
        r'/connexion/',
        r'/login/',
        r'/personnel/',
        r'/prive/'
    ]

    def __init__(self):
        self.robots_cache = {}
        self.rate_limits = {}

    async def check_scraping_legality(self, url: str) -> Tuple[bool, str, Dict]:
        """
        V√©rifie la l√©galit√© du scraping pour une URL donn√©e

        Returns:
            (is_legal, reason, recommendations)
        """
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()

        result = {
            'url': url,
            'domain': domain,
            'checks': {},
            'recommendations': {}
        }

        # 1. V√©rification domaine autoris√©
        is_gov_authorized = self._check_government_authorization(domain)
        result['checks']['government_authorized'] = is_gov_authorized

        # 2. V√©rification robots.txt
        robots_allowed = await self._check_robots_txt(url)
        result['checks']['robots_allowed'] = robots_allowed

        # 3. V√©rification patterns interdits
        safe_patterns = self._check_url_patterns(url)
        result['checks']['safe_patterns'] = safe_patterns

        # 4. Recommandations de rate limiting
        rate_limit = self._get_recommended_rate_limit(domain)
        result['recommendations']['rate_limit'] = rate_limit

        # 5. V√©rification HTTPS
        is_https = parsed_url.scheme == 'https'
        result['checks']['https'] = is_https

        # D√©cision finale
        is_legal = all([
            is_gov_authorized['allowed'],
            robots_allowed['allowed'],
            safe_patterns['safe'],
            is_https
        ])

        reason = self._generate_legality_reason(result)

        return is_legal, reason, result

    def _check_government_authorization(self, domain: str) -> Dict:
        """
        V√©rifie si le domaine est dans la liste des sites gouvernementaux autoris√©s
        """
        if domain in self.AUTHORIZED_GOVERNMENT_SITES:
            site_info = self.AUTHORIZED_GOVERNMENT_SITES[domain]
            return {
                'allowed': True,
                'type': site_info['type'],
                'open_data': site_info['open_data'],
                'notes': site_info['notes']
            }

        # V√©rification des sous-domaines gouvernementaux
        gov_suffixes = ['.gouv.fr', '.fr']
        for suffix in gov_suffixes:
            if domain.endswith(suffix):
                return {
                    'allowed': True,
                    'type': 'government_subdomain',
                    'open_data': True,
                    'notes': f'Sous-domaine gouvernemental fran√ßais ({suffix})'
                }

        return {
            'allowed': False,
            'reason': 'Domaine non autoris√© pour le scraping automatique',
            'recommendation': 'Utiliser l\'upload manuel ou contacter l\'√©diteur'
        }

    async def _check_robots_txt(self, url: str) -> Dict:
        """
        V√©rifie les permissions dans robots.txt
        """
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        robots_url = f"{parsed_url.scheme}://{domain}/robots.txt"

        if domain in self.robots_cache:
            rp = self.robots_cache[domain]
        else:
            try:
                rp = RobotFileParser()
                rp.set_url(robots_url)

                # T√©l√©chargement avec timeout
                async with aiohttp.ClientSession() as session:
                    async with session.get(robots_url, timeout=10) as response:
                        if response.status == 200:
                            robots_content = await response.text()
                            # Parse du contenu robots.txt
                            lines = robots_content.split('\n')
                            # Simulation du parsing (version simplifi√©e)

                self.robots_cache[domain] = rp

            except Exception as e:
                logger.warning(f"Impossible de r√©cup√©rer robots.txt pour {domain}: {e}")
                # En cas d'erreur, on consid√®re comme autoris√© avec prudence
                return {
                    'allowed': True,
                    'reason': 'robots.txt inaccessible, proceeding with caution',
                    'crawl_delay': 2  # D√©lai conservateur
                }

        # V√©rification pour notre user-agent
        user_agent = 'PhoenixCare-Bot/1.0 (+https://phoenixcare.fr/bot)'

        try:
            can_fetch = rp.can_fetch(user_agent, url)
            crawl_delay = rp.crawl_delay(user_agent) or 1

            return {
                'allowed': can_fetch,
                'crawl_delay': crawl_delay,
                'user_agent': user_agent,
                'robots_url': robots_url
            }
        except:
            return {
                'allowed': True,
                'crawl_delay': 2,
                'reason': 'robots.txt parsing failed, proceeding cautiously'
            }

    def _check_url_patterns(self, url: str) -> Dict:
        """
        V√©rifie que l'URL ne contient pas de patterns suspects
        """
        import re

        for pattern in self.FORBIDDEN_PATTERNS:
            if re.search(pattern, url, re.IGNORECASE):
                return {
                    'safe': False,
                    'violated_pattern': pattern,
                    'reason': 'URL contient des √©l√©ments potentiellement priv√©s'
                }

        return {
            'safe': True,
            'reason': 'URL ne contient pas de patterns interdits'
        }

    def _get_recommended_rate_limit(self, domain: str) -> Dict:
        """
        Recommande un rate limiting appropri√©
        """
        if domain in self.AUTHORIZED_GOVERNMENT_SITES:
            return {
                'requests_per_second': 0.5,  # 1 requ√™te toutes les 2 secondes
                'concurrent_requests': 1,
                'respect_server': True,
                'reason': 'Rate limiting respectueux pour sites gouvernementaux'
            }

        return {
            'requests_per_second': 0.2,  # 1 requ√™te toutes les 5 secondes
            'concurrent_requests': 1,
            'respect_server': True,
            'reason': 'Rate limiting tr√®s conservateur pour sites non-gouvernementaux'
        }

    def _generate_legality_reason(self, result: Dict) -> str:
        """
        G√©n√®re une explication de la d√©cision de l√©galit√©
        """
        checks = result['checks']

        if not checks['government_authorized']['allowed']:
            return "‚ùå Domaine non autoris√© - utiliser l'upload manuel"

        if not checks['robots_allowed']['allowed']:
            return "‚ùå Interdit par robots.txt"

        if not checks['safe_patterns']['safe']:
            return "‚ùå URL contient des √©l√©ments priv√©s potentiels"

        if not checks['https']:
            return "‚ö†Ô∏è Site non s√©curis√© (HTTP) - prudence requise"

        return "‚úÖ Scraping autoris√© avec rate limiting respectueux"

class EthicalScraper:
    """
    Scraper √©thique et l√©gal pour PhoenixCare
    """

    def __init__(self):
        self.compliance_checker = LegalComplianceChecker()
        self.session = None

    async def scrape_with_compliance(self, url: str) -> Tuple[bool, str, Optional[str]]:
        """
        Scrape une URL en respectant la conformit√© l√©gale

        Returns:
            (success, content_or_error, compliance_info)
        """
        # V√©rification l√©gale pr√©alable
        is_legal, reason, compliance_info = await self.compliance_checker.check_scraping_legality(url)

        if not is_legal:
            return False, f"Scraping non autoris√©: {reason}", str(compliance_info)

        # Rate limiting
        await self._apply_rate_limiting(url, compliance_info)

        try:
            if not self.session:
                timeout = aiohttp.ClientTimeout(total=30)
                self.session = aiohttp.ClientSession(
                    timeout=timeout,
                    headers={
                        'User-Agent': 'PhoenixCare-Bot/1.0 (+https://phoenixcare.fr/bot)',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.7',
                        'Accept-Encoding': 'gzip, deflate',
                        'DNT': '1',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                    }
                )

            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    return True, content, str(compliance_info)
                else:
                    return False, f"HTTP {response.status}", str(compliance_info)

        except Exception as e:
            return False, f"Erreur scraping: {e}", str(compliance_info)

    async def _apply_rate_limiting(self, url: str, compliance_info: Dict):
        """
        Applique le rate limiting recommand√©
        """
        parsed_url = urlparse(url)
        domain = parsed_url.netloc

        rate_limit = compliance_info['recommendations']['rate_limit']
        delay = 1.0 / rate_limit['requests_per_second']

        # V√©rification du dernier acc√®s √† ce domaine
        last_access = self.compliance_checker.rate_limits.get(domain, 0)
        elapsed = time.time() - last_access

        if elapsed < delay:
            sleep_time = delay - elapsed
            logger.info(f"Rate limiting: attente {sleep_time:.1f}s pour {domain}")
            await asyncio.sleep(sleep_time)

        self.compliance_checker.rate_limits[domain] = time.time()

    async def close(self):
        """
        Ferme la session HTTP
        """
        if self.session:
            await self.session.close()

# Guide de conformit√©
COMPLIANCE_GUIDE = """
üá´üá∑ GUIDE DE CONFORMIT√â L√âGALE - PhoenixCare

‚úÖ SITES AUTORIS√âS:
- Sites gouvernementaux (.gouv.fr)
- Donn√©es publiques officielles
- Sites avec politique d'ouverture des donn√©es

‚ö†Ô∏è R√àGLES √Ä RESPECTER:
1. Rate limiting (max 1 req/2s pour sites gouvernementaux)
2. Respect robots.txt
3. User-agent identifiable
4. Pas de donn√©es personnelles
5. Usage d'int√©r√™t public uniquement

‚ùå SITES INTERDITS:
- Espaces priv√©s/authentifi√©s
- Donn√©es personnelles
- Sites commerciaux sans autorisation
- Contenu prot√©g√© par copyright

üîß ALTERNATIVES L√âGALES:
- APIs officielles quand disponibles
- Upload manuel de documents
- Partenariats avec administrations
- Donn√©es Open Data

üìû CONTACT:
En cas de doute, contacter les administrations
pour obtenir une autorisation explicite.
"""